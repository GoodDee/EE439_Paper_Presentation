{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from arch import arch_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "x = '[1, 2, 3, 4, 5]'\n",
    "json.loads(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICE_FILES = [\"data/Stocks/AlternativeEnergy_Price.csv\", \"data/Stocks/Automobile_Price.csv\", \"data/Stocks/Bank_Price.csv\",\n",
    "                        \"data/Stocks/Beverage_Price.csv\", \"data/Stocks/BioTech_Price.csv\", \"data/Stocks/Chemical_Price.csv\",\n",
    "                        \"data/Stocks/Construction_Price.csv\", \"data/Stocks/Electricity_Price.csv\", \"data/Stocks/Electronic_Price.csv\",\n",
    "                        \"data/Stocks/Finance_Price.csv\", \"data/Stocks/Fix_Price.csv\", \"data/Stocks/Food_Price.csv\", \n",
    "                        \"data/Stocks/FoodProducer_Price.csv\", \"data/Stocks/Gas_Price.csv\", \"data/Stocks/GeneralIndustrial_Price.csv\",\n",
    "                        \"data/Stocks/GeneralRetail_Price.csv\", \"data/Stocks/Hardware_Price.csv\", \"data/Stocks/Health_Price.csv\",\n",
    "                        \"data/Stocks/Household_Price.csv\", \"data/Stocks/IndustrialEngineer_Price.csv\", \"data/Stocks/IndustrialMetal_Price.csv\", \n",
    "                        \"data/Stocks/IndustrialTransport_Price.csv\", \"data/Stocks/Insurance_Price.csv\", \"data/Stocks/Leisure_Price.csv\",\n",
    "                        \"data/Stocks/Media_Price.csv\", \"data/Stocks/Mining_Price.csv\", \"data/Stocks/NonLifeInsure_Price.csv\",\n",
    "                        \"data/Stocks/OilProducer_Price.csv\", \"data/Stocks/Paper_Price.csv\", \"data/Stocks/PersonalGoods_Price.csv\",\n",
    "                        \"data/Stocks/RealEstate_Price.csv\", \"data/Stocks/Software_Price.csv\", \"data/Stocks/Support_Price.csv\",\n",
    "                        \"data/Stocks/Travel_Price.csv\", \"data/Stocks/Unclassified_Price.csv\"]\n",
    "\n",
    "VOLUME_FILES = [\"data/Stocks/AlternativeEnergy_Volume.csv\", \"data/Stocks/Automobile_Volume.csv\", \"data/Stocks/Automobile_Volume.csv\",\n",
    "                           \"data/Stocks/Beverage_Volume.csv\", \"data/Stocks/BioTech_Volume.csv\", \"data/Stocks/Chemical_Volume.csv\",\n",
    "                           \"data/Stocks/Construction_Volume.csv\", \"data/Stocks/Electricity_Volume.csv\", \"data/Stocks/Electronic_Volume.csv\",\n",
    "                           \"data/Stocks/Finance_Volume.csv\", \"data/Stocks/Fix_Volume.csv\", \"data/Stocks/Food_Volume.csv\", \n",
    "                           \"data/Stocks/FoodProducer_Volume.csv\", \"data/Stocks/Gas_Volume.csv\", \"data/Stocks/GeneralIndustrial_Volume.csv\",\n",
    "                           \"data/Stocks/GeneralRetail_Volume.csv\", \"data/Stocks/Hardware_Volume.csv\", \"data/Stocks/Health_Volume.csv\", \n",
    "                           \"data/Stocks/Household_Volume.csv\", \"data/Stocks/IndustrialEngineer_Volume.csv\", \"data/Stocks/IndustrialMetal_Volume.csv\", \n",
    "                           \"data/Stocks/IndustrialTransport_Volume.csv\", \"data/Stocks/Insurance_Volume.csv\", \"data/Stocks/Leisure_Volume.csv\",\n",
    "                           \"data/Stocks/Media_Volume.csv\", \"data/Stocks/Mining_Volume.csv\", \"data/Stocks/NonLifeInsure_Volume.csv\",\n",
    "                           \"data/Stocks/OilProducer_Volume.csv\", \"data/Stocks/Paper_Volume.csv\", \"data/Stocks/PersonalGoods_Volume.csv\",\n",
    "                           \"data/Stocks/RealEstate_Volume.csv\", \"data/Stocks/Software_Volume.csv\", \"data/Stocks/Support_Volume.csv\",\n",
    "                           \"data/Stocks/Travel_Volume.csv\", \"data/Stocks/Unclassified_Volume.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RenameHeader(col_name, col_type):\n",
    "    if col_name == \"Code\":\n",
    "        return \"Code\"\n",
    "    else:\n",
    "        if col_type == \"Price\":\n",
    "            return col_name[2:-3]\n",
    "        else:\n",
    "            return col_name[2:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit 4: Helper Functions\n",
    "def Mean(List):\n",
    "    return sum(List)/len(List)\n",
    "\n",
    "def Median(List):\n",
    "    return np.percentile(List, 50)\n",
    "\n",
    "def Min(List):\n",
    "    return min(List)\n",
    "\n",
    "def Max(List):\n",
    "    return max(List)\n",
    "\n",
    "def Find_25(List):\n",
    "    return np.percentile(List, 25)\n",
    "\n",
    "def Find_75(List):\n",
    "    return np.percentile(List, 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean(price, volume):\n",
    "    \n",
    "    \"\"\"\n",
    "    Argument: price dataframe and volume dataframe\n",
    "    Return: return the tuples of price dataframes (2014-2019) and volume dataframes (2014-2019)\n",
    "                where each one of them is filtered out missing col, missing price from prev year, missing vol from prev year,\n",
    "                        no volume traded occurs from prev year\n",
    "    Note: this function requires RenameHeader (Defined above) and pandas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read two dataframes and filter to have files from 2014 to 2019\n",
    "    price_df = pd.read_csv(price).rename(columns = lambda x: RenameHeader(x, \"Price\"))\n",
    "    price_df['Code'] = pd.to_datetime(price_df['Code'])\n",
    "    price_df = price_df[(price_df['Code'] > '2014-01-01') & (price_df['Code'] < '2020-01-01')]\n",
    "    \n",
    "    vol_df = pd.read_csv(volume).rename(columns = lambda x: RenameHeader(x, \"Vol\"))\n",
    "    vol_df['Code'] = pd.to_datetime(vol_df['Code'])\n",
    "    vol_df = vol_df[(vol_df['Code'] > '2014-01-01') & (vol_df['Code'] < '2020-01-01')]\n",
    "    \n",
    "    # Use only common cols in two dataframes\n",
    "    common_cols = price_df.columns.intersection(vol_df.columns).tolist()\n",
    "    price_df = price_df[common_cols]\n",
    "    vol_df = vol_df[common_cols]\n",
    "    \n",
    "    # Temporarily include SET_VOL to filter holidays\n",
    "    SET_IDX_VOL = pd.read_csv('data/SET/SET_VO.csv', parse_dates = True)\n",
    "    SET_IDX_VOL = SET_IDX_VOL.rename(columns = {'Code': 'Code', 'BNGKSET(VO)': 'Volume'})\n",
    "    SET_IDX_VOL['Code'] = pd.to_datetime(SET_IDX_VOL['Code'])\n",
    "    \n",
    "    # Filter holiday on price dataframes\n",
    "    price_df = pd.merge(price_df, SET_IDX_VOL, how = 'inner', on = 'Code')\n",
    "    price_df = price_df[price_df['Volume'].notna()]\n",
    "    price_df.drop(['Volume'], axis = 1, inplace = True)\n",
    "    \n",
    "    # Filter holiday on price dataframes\n",
    "    vol_df = pd.merge(vol_df, SET_IDX_VOL, how = 'inner', on = 'Code')\n",
    "    vol_df = vol_df[vol_df['Volume'].notna()]\n",
    "    vol_df.drop(['Volume'], axis = 1, inplace = True)\n",
    "    \n",
    "    # Create two dataframes\n",
    "    price_df_s = []\n",
    "    vol_df_s = []\n",
    "    price_df_s.append(price_df[(price_df['Code'] > '2014-01-01') & (price_df['Code'] < '2015-01-01')])\n",
    "    vol_df_s.append(vol_df[(vol_df['Code'] > '2014-01-01') & (vol_df['Code'] < '2015-01-01')])\n",
    "    price_df_s.append(price_df[(price_df['Code'] > '2015-01-01') & (price_df['Code'] < '2020-01-01')])\n",
    "    vol_df_s.append(vol_df[(vol_df['Code'] > '2015-01-01') & (vol_df['Code'] < '2020-01-01')])    \n",
    "    \n",
    "        \n",
    "    # Filter the columns by\n",
    "    # 1) Any missing + zero volume from previous year\n",
    "    for i in range(1):\n",
    "        \n",
    "        price_filter_df = price_df_s[i+1]\n",
    "        vol_filter_df = vol_df_s[i+1]\n",
    "        \n",
    "        price_null = price_filter_df.columns[price_filter_df.isna().any()].tolist()\n",
    "        vol_null = vol_filter_df.columns[vol_filter_df.isna().any()].tolist()\n",
    "        # Create another copy (a bit inefficient, but it works)\n",
    "        vol_temp = vol_filter_df.drop(['Code'], axis = 1, inplace = False)\n",
    "        vol_gt_zero = vol_temp.columns[(vol_temp <= 0).any()].tolist()\n",
    "        \n",
    "        filtered_out_col = list(set().union(price_null,vol_null,vol_gt_zero))\n",
    "        \n",
    "        price_df_s[i+1].drop(filtered_out_col, axis = 1, inplace = True)\n",
    "        vol_df_s[i+1].drop(filtered_out_col, axis = 1, inplace = True)\n",
    "        \n",
    "        \n",
    "    return (price_df_s, vol_df_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateFeatures(price_list, vol_list):\n",
    "    \"\"\"\n",
    "    Input: price dataframes list, vol dataframes list\n",
    "    Return: list of list of dict\n",
    "    (for example, [automobile, ...] -> [2014, 2015, 2016, ...] -> {price: price_df, vol: vol_df, feature1: ...})\n",
    "    \"\"\"\n",
    "    return_list = []\n",
    "    for i in range(len(price_list)):\n",
    "        compile_list = []\n",
    "        price_df_s, vol_df_s = Clean(price_list[i], vol_list[i])\n",
    "        \n",
    "        for j in range(len(price_df_s)):\n",
    "            sub_dict = {}\n",
    "            price_df = price_df_s[j]\n",
    "            vol_df = vol_df_s[j]\n",
    "            \n",
    "            ## Define new dataframes: return dataframe\n",
    "            return1_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            return1_df['Code'] = price_df['Code']\n",
    "            \n",
    "            return3_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            return3_df['Code'] = price_df['Code']\n",
    "            \n",
    "            return5_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            return5_df['Code'] = price_df['Code']    \n",
    "            \n",
    "            return10_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            return10_df['Code'] = price_df['Code']\n",
    "            \n",
    "            return14_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            return14_df['Code'] = price_df['Code']\n",
    "            \n",
    "            return20_df = pd.DataFrame(index=price_df.index, columns=price_df.columns)\n",
    "            return20_df['Code'] = price_df['Code']\n",
    "            \n",
    "            for col in list(price_df.columns):\n",
    "                if col != 'Code':\n",
    "                    # Calculate log return here\n",
    "                    return1_df[col] = 100*(np.log(price_df.loc[:, col]) - np.log(price_df.loc[:, col].shift(1)))\n",
    "                    return3_df[col] = 100*(np.log(price_df.loc[:, col]) - np.log(price_df.loc[:, col].shift(3)))\n",
    "                    return5_df[col] = 100*(np.log(price_df.loc[:, col]) - np.log(price_df.loc[:, col].shift(5)))\n",
    "                    return10_df[col] = 100*(np.log(price_df.loc[:, col]) - np.log(price_df.loc[:, col].shift(10)))\n",
    "                    return14_df[col] = 100*(np.log(price_df.loc[:, col]) - np.log(price_df.loc[:, col].shift(14)))\n",
    "                    return20_df[col] = 100*(np.log(price_df.loc[:, col]) - np.log(price_df.loc[:, col].shift(20)))\n",
    "                    \n",
    "                    #return3_df[col] = 100*price_df.loc[:, col].pct_change(periods = 3)\n",
    "                    #return5_df[col] = 100*price_df.loc[:, col].pct_change(periods = 5)\n",
    "                    #return10_df[col] = 100*price_df.loc[:, col].pct_change(periods = 10)\n",
    "                    #return14_df[col] = 100*price_df.loc[:, col].pct_change(periods = 14)\n",
    "                    #return20_df[col] = 100*price_df.loc[:, col].pct_change(periods = 20)\n",
    "                    \n",
    "            \n",
    "            sub_dict['price'] = price_df\n",
    "            sub_dict['vol'] = vol_df\n",
    "            sub_dict['ret_1'] = return1_df\n",
    "            sub_dict['ret_3'] = return3_df\n",
    "            sub_dict['ret_5'] = return5_df     \n",
    "            sub_dict['ret_10'] = return10_df\n",
    "            sub_dict['ret_14'] = return14_df\n",
    "            sub_dict['ret_20'] = return20_df\n",
    "\n",
    "            compile_list.append(sub_dict)\n",
    "            \n",
    "        return_list.append(compile_list)\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rearrange(sector_list):\n",
    "    \"\"\"\n",
    "    Input: sector_list (return list from GenerateFeatures function above)\n",
    "    Return: list of list of dict as follows\n",
    "        [2014, 2015, 2016, ...] -> {key: PTT, data: dataframes}\n",
    "        (similar how we usually render in React)\n",
    "    \"\"\"\n",
    "    # Each sublist is for each year\n",
    "    return_list = [[], []]\n",
    "    \n",
    "    # i runs from 0 to # of sectors - 1\n",
    "    for i in range(len(sector_list)):\n",
    "        sector_stocks = sector_list[i]\n",
    "        # j runs from 0 to 5 (0 -> 2014, 5 -> 2019)\n",
    "        for j in range(2):\n",
    "            sector_year_dict = sector_stocks[j]\n",
    "            col_names = sector_year_dict['price'].columns.tolist()\n",
    "            # k runs for each column except \"Code\"\n",
    "            for k in range(1, len(col_names)):\n",
    "                stock_name = col_names[k]\n",
    "                # Step 1: create blank dataframes\n",
    "                new_data = pd.DataFrame(index=sector_year_dict['price'].index, columns= ['Code', 'vol', 'price', 'ret_3', 'ret_5', 'ret_10', 'ret_14', 'ret_20'])\n",
    "                new_data['Code'] = sector_year_dict['price']['Code']\n",
    "                # Step 2: Retrieve data from the key and insert it into new_data\n",
    "                new_data['vol'] = sector_year_dict['vol'].loc[:, stock_name]\n",
    "                new_data['price'] = sector_year_dict['price'].loc[:, stock_name]\n",
    "                new_data['ret_1'] = sector_year_dict['ret_1'].loc[:, stock_name]\n",
    "                new_data['ret_3'] = sector_year_dict['ret_3'].loc[:, stock_name]\n",
    "                new_data['ret_5'] = sector_year_dict['ret_5'].loc[:, stock_name]\n",
    "                new_data['ret_10'] = sector_year_dict['ret_10'].loc[:, stock_name]\n",
    "                new_data['ret_14'] = sector_year_dict['ret_14'].loc[:, stock_name]\n",
    "                new_data['ret_20'] = sector_year_dict['ret_20'].loc[:, stock_name]\n",
    "                \n",
    "                # Step 3: Drop rows with at least one missing value\n",
    "                new_data.dropna(axis = 0, how = 'any', inplace = True)\n",
    "                # Step 4: Insert the new_dataframe into appropriate place (notice we use dict for convenience later)\n",
    "                new_dict = {'key': stock_name, 'data': new_data}\n",
    "                return_list[j].append(new_dict)\n",
    "                \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVolumeTraded(RenderList):\n",
    "    \"\"\"\n",
    "    input: RenderList, using the same data structure corresponding to Rearrange function (defined above)\n",
    "    Process: Add key of volume_traded in each stock-year (calculated from last year)\n",
    "    Return: nothing\n",
    "    \"\"\"\n",
    "    # For each year from 2015 to 2019\n",
    "    for i in range(1, 2):\n",
    "        \n",
    "        # For each stock that we want to add the new key: volume traded\n",
    "        for stock in RenderList[i]:\n",
    "            stock['volume_traded'] = stock['data'].loc[:, 'vol'].mean()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNoKey(RenderList, key):\n",
    "    remove_keys = []\n",
    "    \n",
    "    for entry in RenderList:\n",
    "        if key not in entry:\n",
    "            remove_keys.append(entry[\"key\"]) # Append stock abbreviations we want to remove\n",
    "            \n",
    "    if len(remove_keys) == 0:\n",
    "        return RenderList\n",
    "    else:\n",
    "        return [x for x in RenderList if x[\"key\"] not in remove_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveInsigLM(RenderList):\n",
    "    \"\"\"\n",
    "    Input: RenderList, using the same data structure corresponding to Rearrange function\n",
    "    This will filter only stocks where LM test is rejected: there is an ARCH effect\n",
    "    Return: nothing\n",
    "    \"\"\"\n",
    "    columns = ['Number_of_Firms', 'Sig_LM_5%', 'log_ret']\n",
    "    return1_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return1_df = return1_df.fillna(0)\n",
    "    \n",
    "    return3_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return3_df = return3_df.fillna(0)\n",
    "    \n",
    "    return5_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return5_df = return5_df.fillna(0)\n",
    "    \n",
    "    return10_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return10_df = return10_df.fillna(0)\n",
    "    \n",
    "    return14_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return14_df = return14_df.fillna(0)\n",
    "    \n",
    "    return20_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return20_df = return20_df.fillna(0)\n",
    "    \n",
    "    for i in range(1, 2):\n",
    "        # For each year, split stocks into ten deciles based on previous volume traded\n",
    "        stock_list = RemoveNoKey(RenderList[i], 'volume_traded')\n",
    "        stock_list = sorted(stock_list, key = lambda x: x['volume_traded'])\n",
    "        ten_splits = np.array_split(stock_list, 10)\n",
    "        \n",
    "        for j in range(10):\n",
    "            \n",
    "            # Record the number of firms \n",
    "            return1_df.loc[j, 'Number_of_Firms'] += len(ten_splits[j])\n",
    "            return3_df.loc[j, 'Number_of_Firms'] += len(ten_splits[j])\n",
    "            return5_df.loc[j, 'Number_of_Firms'] += len(ten_splits[j])\n",
    "            return10_df.loc[j, 'Number_of_Firms'] += len(ten_splits[j])\n",
    "            return14_df.loc[j, 'Number_of_Firms'] += len(ten_splits[j])\n",
    "            return20_df.loc[j, 'Number_of_Firms'] += len(ten_splits[j])\n",
    "            \n",
    "            # Accumulator\n",
    "            significant_LM_1,  significant_LM_3, significant_LM_5, significant_LM_10, significant_LM_14, significant_LM_20 = 0, 0, 0, 0, 0, 0\n",
    "            returns = [0, 0, 0, 0, 0, 0]\n",
    "            \n",
    "            for Render in ten_splits[j]:\n",
    "                \n",
    "                stock_df = Render['data']\n",
    "                \n",
    "                returns[0] += stock_df['ret_1'].mean()\n",
    "                returns[1] += stock_df['ret_3'].mean()\n",
    "                returns[2] += stock_df['ret_5'].mean()\n",
    "                returns[3] += stock_df['ret_10'].mean()\n",
    "                returns[4] += stock_df['ret_14'].mean()\n",
    "                returns[5] += stock_df['ret_20'].mean()\n",
    "                \n",
    "                # Check for significance\n",
    "                if (sm.stats.diagnostic.het_arch(stock_df['ret_1']))[1] <= 0.05:\n",
    "                    significant_LM_1 += 1\n",
    "                    Render['LM_sig_1'] = True\n",
    "                \n",
    "                if (sm.stats.diagnostic.het_arch(stock_df['ret_3']))[1] <= 0.05:\n",
    "                    significant_LM_3 += 1\n",
    "                    Render['LM_sig_3'] = True\n",
    "                    \n",
    "                if (sm.stats.diagnostic.het_arch(stock_df['ret_5']))[1] <= 0.05:\n",
    "                    significant_LM_5 += 1\n",
    "                    Render['LM_sig_5'] = True\n",
    "                    \n",
    "                if (sm.stats.diagnostic.het_arch(stock_df['ret_10']))[1] <= 0.05:\n",
    "                    significant_LM_10 += 1\n",
    "                    Render['LM_sig_10'] = True\n",
    "                    \n",
    "                if (sm.stats.diagnostic.het_arch(stock_df['ret_14']))[1] <= 0.05:\n",
    "                    significant_LM_14 += 1\n",
    "                    Render['LM_sig_14'] = True\n",
    "                    \n",
    "                if (sm.stats.diagnostic.het_arch(stock_df['ret_20']))[1] <= 0.05:\n",
    "                    significant_LM_20 += 1\n",
    "                    Render['LM_sig_20'] = True\n",
    "                    \n",
    "            return1_df.loc[j,  'Sig_LM_5%'] += significant_LM_1\n",
    "            return3_df.loc[j,  'Sig_LM_5%'] += significant_LM_3\n",
    "            return5_df.loc[j,  'Sig_LM_5%'] += significant_LM_5\n",
    "            return10_df.loc[j,  'Sig_LM_5%'] += significant_LM_10        \n",
    "            return14_df.loc[j,  'Sig_LM_5%'] += significant_LM_14\n",
    "            return20_df.loc[j,  'Sig_LM_5%'] += significant_LM_20   \n",
    "            \n",
    "            return1_df.loc[j,  'log_ret'] += returns[0]/len(ten_splits[j])\n",
    "            return3_df.loc[j,  'log_ret'] += returns[1]/len(ten_splits[j])\n",
    "            return5_df.loc[j,  'log_ret'] += returns[2]/len(ten_splits[j])\n",
    "            return10_df.loc[j,  'log_ret'] += returns[3]/len(ten_splits[j])    \n",
    "            return14_df.loc[j, 'log_ret'] += returns[4]/len(ten_splits[j])\n",
    "            return20_df.loc[j, 'log_ret'] += returns[5]/len(ten_splits[j])             \n",
    "\n",
    "    # Get average value\n",
    "    return1_df.to_csv('ARCH_Test_Pool_1.csv')\n",
    "    return3_df.to_csv('ARCH_Test_Pool_3.csv')\n",
    "    return5_df.to_csv('ARCH_Test_Pool_5.csv')\n",
    "    return10_df.to_csv('ARCH_Test_Pool_10.csv')\n",
    "    return14_df.to_csv('ARCH_Test_Pool_14.csv')\n",
    "    return20_df.to_csv('ARCH_Test_Pool_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTable_GARCH(RenderList):\n",
    "    \"\"\"\n",
    "    Input: RenderList, using the same data structure corresponding to Rearrange function and has the key volume_traded\n",
    "    This will output the result of average coefficient across five years into csv file\n",
    "    Return: nothing\n",
    "    \"\"\"\n",
    "    # This is the return dataframe with three columns corresponding to each coefficient\n",
    "    \n",
    "   # Edit 1: Enable to record mean, min, 25%, median, 75%, max\n",
    "    temp_col = ['GARCH_alpha',  'GARCH_beta', 'GJR_alpha', 'GJR_beta', 'GJR_gamma']\n",
    "    columns = temp_col\n",
    "    #for var in temp_col:\n",
    "    #    columns.append((var + '_mean'))\n",
    "    #    columns.append((var + '_min'))\n",
    "    #    columns.append((var + '_25'))\n",
    "     #   columns.append((var + '_median'))\n",
    "     #   columns.append((var + '_75'))\n",
    "    #    columns.append((var + '_max'))\n",
    "    \n",
    "    return1_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return1_df = return1_df.fillna(0)    \n",
    "    \n",
    "    return3_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return3_df = return3_df.fillna(0)\n",
    "    \n",
    "    return5_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return5_df = return5_df.fillna(0)\n",
    "    \n",
    "    return10_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return10_df = return10_df.fillna(0)\n",
    "    \n",
    "    return14_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return14_df = return14_df.fillna(0)\n",
    "    \n",
    "    return20_df = pd.DataFrame(index=range(10) , columns= columns)\n",
    "    return20_df = return20_df.fillna(0)\n",
    "    \n",
    "    for i in range(1, 2):\n",
    "        # Sort by volume traded and split into ten sub-lists\n",
    "        \n",
    "        stock_list_1 = RemoveNoKey(RenderList[i], 'LM_sig_1')\n",
    "        stock_list_1 = sorted(stock_list_1, key = lambda x: x['volume_traded'])\n",
    "        ten_splits_1 = np.array_split(stock_list_1, 10)\n",
    "        \n",
    "        stock_list_3 = RemoveNoKey(RenderList[i], 'LM_sig_3')\n",
    "        stock_list_3 = sorted(stock_list_3, key = lambda x: x['volume_traded'])\n",
    "        ten_splits_3 = np.array_split(stock_list_3, 10)\n",
    "        \n",
    "        stock_list_5 = RemoveNoKey(RenderList[i], 'LM_sig_5')    \n",
    "        stock_list_5 = sorted(stock_list_5, key = lambda x: x['volume_traded'])\n",
    "        ten_splits_5 = np.array_split(stock_list_5, 10)\n",
    "        \n",
    "        stock_list_10 = RemoveNoKey(RenderList[i], 'LM_sig_10')\n",
    "        stock_list_10 = sorted(stock_list_10, key = lambda x: x['volume_traded'])\n",
    "        ten_splits_10 = np.array_split(stock_list_10, 10)\n",
    "        \n",
    "        stock_list_14 = RemoveNoKey(RenderList[i], 'LM_sig_14')\n",
    "        stock_list_14 = sorted(stock_list_14, key = lambda x: x['volume_traded'])\n",
    "        ten_splits_14 = np.array_split(stock_list_14, 10)\n",
    "        \n",
    "        stock_list_20 = RemoveNoKey(RenderList[i], 'LM_sig_20')\n",
    "        stock_list_20 = sorted(stock_list_20, key = lambda x: x['volume_traded'])\n",
    "        ten_splits_20 = np.array_split(stock_list_20, 10)\n",
    "        \n",
    "        for j in range(10):\n",
    "            \n",
    "            # Edit 2: Accumulator: record each coeff in each sub-list\n",
    "            ret1_coeffs = [[] for x in range(5)]\n",
    "            ret3_coeffs = [[] for x in range(5)]\n",
    "            ret5_coeffs = [[] for x in range(5)]\n",
    "            ret10_coeffs = [[] for x in range(5)]\n",
    "            ret14_coeffs = [[] for x in range(5)]\n",
    "            ret20_coeffs = [[] for x in range(5)]\n",
    "            \n",
    "            for Render in ten_splits_1[j]:\n",
    "                \n",
    "                stock_df = Render['data']\n",
    "                \n",
    "                GARCH = arch_model(stock_df['ret_1'], p=1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                GJR_GARCH = arch_model(stock_df['ret_1'], p=1, o = 1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                \n",
    "                # Edit 3: Append each coeff in the assigned sub-list\n",
    "                ret1_coeffs[0].append(GARCH.params['alpha[1]'])\n",
    "                ret1_coeffs[1].append(GARCH.params['beta[1]'])\n",
    "                ret1_coeffs[2].append(GJR_GARCH.params['alpha[1]'])\n",
    "                ret1_coeffs[3].append(GJR_GARCH.params['beta[1]'])\n",
    "                ret1_coeffs[4].append(GJR_GARCH.params['gamma[1]'])\n",
    "                \n",
    "            for k in range(len(temp_col)):\n",
    "                # temp_col[0] = 'GARCH_alpha'\n",
    "                # ret1_coeffs[0] = the list containing each coeff\n",
    "                \n",
    "                return1_df.loc[j, temp_col[k]] = str(ret1_coeffs[k])\n",
    "                \n",
    "                #return1_df.loc[j, temp_col[k]+'_min'] = Min(ret1_coeffs[k])\n",
    "                #return1_df.loc[j, temp_col[k]+'_25'] = Find_25(ret1_coeffs[k])\n",
    "               # return1_df.loc[j, temp_col[k]+'_median'] = Median(ret1_coeffs[k])\n",
    "                #return1_df.loc[j, temp_col[k]+'_75'] = Find_75(ret1_coeffs[k])\n",
    "                #return1_df.loc[j, temp_col[k]+'_max'] = Max(ret1_coeffs[k])\n",
    "            \n",
    "            \n",
    "            for Render in ten_splits_3[j]:\n",
    "                \n",
    "                stock_df = Render['data']\n",
    "                \n",
    "                GARCH = arch_model(stock_df['ret_3'], p=1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                GJR_GARCH = arch_model(stock_df['ret_3'], p=1, o = 1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                \n",
    "                ret3_coeffs[0].append(GARCH.params['alpha[1]'])\n",
    "                ret3_coeffs[1].append(GARCH.params['beta[1]'])\n",
    "                ret3_coeffs[2].append(GJR_GARCH.params['alpha[1]'])\n",
    "                ret3_coeffs[3].append(GJR_GARCH.params['beta[1]'])\n",
    "                ret3_coeffs[4].append(GJR_GARCH.params['gamma[1]'])\n",
    "                \n",
    "            for k in range(len(temp_col)):\n",
    "                # temp_col[0] = 'GARCH_alpha'\n",
    "                # ret1_coeffs[0] = the list containing each coeff\n",
    "                return3_df.loc[j, temp_col[k]] = str(ret3_coeffs[k])\n",
    "                #return3_df.loc[j, temp_col[k]+'_mean'] = Mean(ret3_coeffs[k])\n",
    "                #return3_df.loc[j, temp_col[k]+'_min'] = Min(ret3_coeffs[k])\n",
    "                #return3_df.loc[j, temp_col[k]+'_25'] = Find_25(ret3_coeffs[k])\n",
    "                #return3_df.loc[j, temp_col[k]+'_median'] = Median(ret3_coeffs[k])\n",
    "                #return3_df.loc[j, temp_col[k]+'_75'] = Find_75(ret3_coeffs[k])\n",
    "                #return3_df.loc[j, temp_col[k]+'_max'] = Max(ret3_coeffs[k])     \n",
    "\n",
    "            for Render in ten_splits_5[j]:\n",
    "                \n",
    "                stock_df = Render['data']\n",
    "                \n",
    "                GARCH = arch_model(stock_df['ret_5'], p=1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                GJR_GARCH = arch_model(stock_df['ret_5'], p=1, o = 1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                \n",
    "                ret5_coeffs[0].append(GARCH.params['alpha[1]'])\n",
    "                ret5_coeffs[1].append(GARCH.params['beta[1]'])\n",
    "                ret5_coeffs[2].append(GJR_GARCH.params['alpha[1]'])\n",
    "                ret5_coeffs[3].append(GJR_GARCH.params['beta[1]'])\n",
    "                ret5_coeffs[4].append(GJR_GARCH.params['gamma[1]'])\n",
    "                \n",
    "            for k in range(len(temp_col)):\n",
    "                # temp_col[0] = 'GARCH_alpha'\n",
    "                # ret1_coeffs[0] = the list containing each coeff\n",
    "                return5_df.loc[j, temp_col[k]] = str(ret5_coeffs[k])\n",
    "                #return5_df.loc[j, temp_col[k]+'_mean'] = Mean(ret5_coeffs[k])\n",
    "                #return5_df.loc[j, temp_col[k]+'_min'] = Min(ret5_coeffs[k])\n",
    "                #return5_df.loc[j, temp_col[k]+'_25'] = Find_25(ret5_coeffs[k])\n",
    "                #return5_df.loc[j, temp_col[k]+'_median'] = Median(ret5_coeffs[k])\n",
    "                #return5_df.loc[j, temp_col[k]+'_75'] = Find_75(ret5_coeffs[k])\n",
    "                #return5_df.loc[j, temp_col[k]+'_max'] = Max(ret5_coeffs[k])                  \n",
    "            \n",
    "            for Render in ten_splits_10[j]:\n",
    "                \n",
    "                stock_df = Render['data']\n",
    "                \n",
    "                GARCH = arch_model(stock_df['ret_10'], p=1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                GJR_GARCH = arch_model(stock_df['ret_10'], p=1, o = 1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                \n",
    "                ret10_coeffs[0].append(GARCH.params['alpha[1]'])\n",
    "                ret10_coeffs[1].append(GARCH.params['beta[1]'])\n",
    "                ret10_coeffs[2].append(GJR_GARCH.params['alpha[1]'])\n",
    "                ret10_coeffs[3].append(GJR_GARCH.params['beta[1]'])\n",
    "                ret10_coeffs[4].append(GJR_GARCH.params['gamma[1]'])\n",
    "                \n",
    "            for k in range(len(temp_col)):\n",
    "                # temp_col[0] = 'GARCH_alpha'\n",
    "                # ret1_coeffs[0] = the list containing each coeff\n",
    "                return10_df.loc[j, temp_col[k]] = str(ret10_coeffs[k])\n",
    "                #return10_df.loc[j, temp_col[k]+'_mean'] = Mean(ret10_coeffs[k])\n",
    "                #return10_df.loc[j, temp_col[k]+'_min'] = Min(ret10_coeffs[k])\n",
    "                #return10_df.loc[j, temp_col[k]+'_25'] = Find_25(ret10_coeffs[k])\n",
    "                #return10_df.loc[j, temp_col[k]+'_median'] = Median(ret10_coeffs[k])\n",
    "                #return10_df.loc[j, temp_col[k]+'_75'] = Find_75(ret10_coeffs[k])\n",
    "                #return10_df.loc[j, temp_col[k]+'_max'] = Max(ret10_coeffs[k])         \n",
    "            \n",
    "            for Render in ten_splits_14[j]:\n",
    "                \n",
    "                stock_df = Render['data']\n",
    "                \n",
    "                GARCH = arch_model(stock_df['ret_14'], p=1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                GJR_GARCH = arch_model(stock_df['ret_14'], p=1, o = 1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                \n",
    "                ret14_coeffs[0].append(GARCH.params['alpha[1]'])\n",
    "                ret14_coeffs[1].append(GARCH.params['beta[1]'])\n",
    "                ret14_coeffs[2].append(GJR_GARCH.params['alpha[1]'])\n",
    "                ret14_coeffs[3].append(GJR_GARCH.params['beta[1]'])\n",
    "                ret14_coeffs[4].append(GJR_GARCH.params['gamma[1]'])\n",
    "                \n",
    "            for k in range(len(temp_col)):\n",
    "                # temp_col[0] = 'GARCH_alpha'\n",
    "                # ret1_coeffs[0] = the list containing each coeff\n",
    "                return14_df.loc[j, temp_col[k]] = str(ret14_coeffs[k])\n",
    "                #return14_df.loc[j, temp_col[k]+'_mean'] = Mean(ret14_coeffs[k])\n",
    "                #return14_df.loc[j, temp_col[k]+'_min'] = Min(ret14_coeffs[k])\n",
    "                #return14_df.loc[j, temp_col[k]+'_25'] = Find_25(ret14_coeffs[k])\n",
    "                #return14_df.loc[j, temp_col[k]+'_median'] = Median(ret14_coeffs[k])\n",
    "                #return14_df.loc[j, temp_col[k]+'_75'] = Find_75(ret14_coeffs[k])\n",
    "                #return14_df.loc[j, temp_col[k]+'_max'] = Max(ret14_coeffs[k])   \n",
    "            \n",
    "            for Render in ten_splits_20[j]:\n",
    "                \n",
    "                stock_df = Render['data']\n",
    "                \n",
    "                GARCH = arch_model(stock_df['ret_20'], p=1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                GJR_GARCH = arch_model(stock_df['ret_20'], p=1, o = 1, q=1).fit(update_freq=5, disp = 'off')\n",
    "                \n",
    "                ret20_coeffs[0].append(GARCH.params['alpha[1]'])\n",
    "                ret20_coeffs[1].append(GARCH.params['beta[1]'])\n",
    "                ret20_coeffs[2].append(GJR_GARCH.params['alpha[1]'])\n",
    "                ret20_coeffs[3].append(GJR_GARCH.params['beta[1]'])\n",
    "                ret20_coeffs[4].append(GJR_GARCH.params['gamma[1]'])\n",
    "                \n",
    "            for k in range(len(temp_col)):\n",
    "                # temp_col[0] = 'GARCH_alpha'\n",
    "                # ret1_coeffs[0] = the list containing each coeff\n",
    "                return20_df.loc[j, temp_col[k]] = str(ret20_coeffs[k])\n",
    "                #return20_df.loc[j, temp_col[k]+'_mean'] = Mean(ret20_coeffs[k])\n",
    "                #return20_df.loc[j, temp_col[k]+'_min'] = Min(ret20_coeffs[k])\n",
    "                #return20_df.loc[j, temp_col[k]+'_25'] = Find_25(ret20_coeffs[k])\n",
    "                #return20_df.loc[j, temp_col[k]+'_median'] = Median(ret20_coeffs[k])\n",
    "                #return20_df.loc[j, temp_col[k]+'_75'] = Find_75(ret20_coeffs[k])\n",
    "                #return20_df.loc[j, temp_col[k]+'_max'] = Max(ret20_coeffs[k])           \n",
    "            \n",
    "    # Get average value\n",
    "    return1_df.to_csv('Full_Model_1_raw.csv')\n",
    "    return3_df.to_csv('Full_Model_3_raw.csv') \n",
    "    return5_df.to_csv('Full_Model_5_raw.csv')\n",
    "    return10_df.to_csv('Full_Model_10_raw.csv')\n",
    "    return14_df.to_csv('Full_Model_14_raw.csv')\n",
    "    return20_df.to_csv('Full_Model_20_raw.csv')\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\statsmodels\\stats\\diagnostic.py:625: FutureWarning: The default value of nlags is changing.  After 0.12, this value will become min(10, nobs//5). Directly setmaxlags or period to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cleaned_dfs = GenerateFeatures(PRICE_FILES, VOLUME_FILES)\n",
    "RenderList = Rearrange(cleaned_dfs)\n",
    "GetVolumeTraded(RenderList)\n",
    "RemoveInsigLM(RenderList)\n",
    "GenerateTable_GARCH(RenderList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Volumes = [x['volume_traded'] for x in RenderList[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([186.,  21.,   9.,   2.,   4.,   3.,   0.,   0.,   0.,   0.,   1.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([4.67609675e+01, 1.25907897e+04, 2.51348185e+04, 3.76788473e+04,\n",
       "        5.02228760e+04, 6.27669048e+04, 7.53109335e+04, 8.78549623e+04,\n",
       "        1.00398991e+05, 1.12943020e+05, 1.25487049e+05, 1.38031077e+05,\n",
       "        1.50575106e+05, 1.63119135e+05, 1.75663164e+05, 1.88207192e+05,\n",
       "        2.00751221e+05, 2.13295250e+05, 2.25839279e+05, 2.38383307e+05,\n",
       "        2.50927336e+05]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n_bins = 20\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.xlabel('Volume Traded (thousands)')\n",
    "plt.ylabel('Frequency')\n",
    "ax.hist(Volumes, bins=n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
